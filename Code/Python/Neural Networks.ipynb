{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from keras.layers import Input, Dense, Lambda, concatenate, Conv1D, Concatenate, Flatten, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up variables for constants such as absolute datapaths and the desired valdiation fraction split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"E:\\\\Development Project\\\\Data\\\\GNPS Python Master\\\\Final Data.txt\"\n",
    "datapath_with_fragments = \"E:\\\\Development Project\\\\Data\\\\GNPS Python Master\\\\Final Data With Fragments.txt\"\n",
    "fingerprints_path = \"E:\\\\Development Project\\\\Data\\\\GNPS Python Master\\\\Final Fingerprints.txt\"\n",
    "fingerprints_names_path = \"E:\\\\Development Project\\\\Data\\\\GNPS Python Master\\\\Fingerprint Legend.txt\"\n",
    "num_samples = 5770\n",
    "num_val_samples = 1000\n",
    "numFeatures = 2995\n",
    "val_fraction = 0.1\n",
    "default_dpi = plt.rcParamsDefault['figure.dpi']\n",
    "plt.rcParams['figure.dpi'] = default_dpi*1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are methods used to load in fragment spectra and fingerprint data from files stored in the absolute paths specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a master file containing peak intensities for all molecules.\n",
    "# Each molecule's spectrum is added as a 1000 element row to a Pandas dataframe\n",
    "# The dataframe is then converted into a numpy array for use as Keras Input.\n",
    "# Include the option of adding additonal features to each molecule (mass_shifts variable)\n",
    "def load_master_file(path, mass_shifts = 0):\n",
    "    MAX_MASS = 1000\n",
    "    BIN_SIZE = 1\n",
    "    NUM_FEATURES = mass_shifts\n",
    "    \n",
    "    mol_all = np.loadtxt(path, dtype=\"U25\") # Get master file in as numpy array\n",
    "    \n",
    "    mol_ids = np.unique(mol_all[:, 0])  # Trim duplicate filename rows, store unique filenames\n",
    "    \n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in mol_ids).\n",
    "    intensities = pd.DataFrame(0.0, index = mol_ids, columns=range((MAX_MASS//BIN_SIZE)+NUM_FEATURES), dtype=float)\n",
    "    print(intensities.index)\n",
    "    \n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in mol_all:\n",
    "        intensities.at[row[0], float(row[1])-1] = float(row[2])\n",
    "    \n",
    "    # Convert populated dataframe into a numpy array for use by neural networks.\n",
    "    np_matrix = intensities.values\n",
    "    return np_matrix\n",
    "\n",
    "# Load a master file containing CDK fingerprints for all molecules.\n",
    "# Each molecules CDK bit set is added as a 320 element array to a Pandas dataframe.\n",
    "def load_fingerprints_master():\n",
    "    BITS = 320  # Total number of bits in fingerprint\n",
    "\n",
    "    fp_all = np.loadtxt(fingerprints_path, dtype=\"U25\") # Get master file as numpy array of Strings\n",
    "    fp_ids = np.unique(fp_all[:, 0]) # Trim duplicate filename rows, store unique filenames\n",
    "\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in fp_ids).\n",
    "    fingerprints = pd.DataFrame(0, index = fp_ids, columns=range(BITS), dtype=int)\n",
    "\n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in fp_all:\n",
    "        fingerprints.at[row[0], int(row[1])] = int(row[2])\n",
    "\n",
    "    # Convert populated dataframe into a numpy array for use as output by neural networks.\n",
    "    np_matrix = fingerprints.values\n",
    "    return np_matrix\n",
    "\n",
    "# Load the names of all substructures included in the CDK fingerprint in the correct order\n",
    "# This is used for boxplots, when performance metrics for individual substructures are calculated.\n",
    "def load_fingerprint_legend():\n",
    "    fingerprint_legend = []\n",
    "    # Open file containing substructure names.\n",
    "    with open(fingerprints_names_path, 'r') as f:\n",
    "        # Add each name to the list of substructure names.\n",
    "        lines = list(islice(f, 0, None))\n",
    "        for line in lines:\n",
    "            fingerprint_legend.append(line)\n",
    "    return fingerprint_legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below methods create and train various neural networks when provided with valid input and output data. They allow for specifying the number of epochs the network is to be trained for and, in some cases, the learning rate. Trained models are returned and can be used to predict on test data and thereby be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sotchastic Gradient Descent object from Keras to allow for tweaking its learning rate.\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# A deep autoencoder which learns to reconstruct fragment spectra.\n",
    "# Structure: (Input)1000-256-128-16-128-256-1000(Output)\n",
    "def basic_autoencoder(x_train, epochs=100, encoded_dim=10):\n",
    "    # Create input based on the provided x_train data structure.\n",
    "    inputLayer = Input(shape=(x_train.shape[1],))  # fixed\n",
    "    # Since an autoencoder reconstructs input, output will have the same shape as input.\n",
    "    output_dims = x_train.shape[1]\n",
    "\n",
    "    # Create the encoding layers using Keras' functional API.\n",
    "    l = inputLayer\n",
    "    l = Dense(256, activation='relu')(l)\n",
    "    l = Dense(128, activation='relu')(l)\n",
    "    l = Dense(16, activation='relu')(l)\n",
    "    \n",
    "    # Create reference to latent space in case we need it later for interpolation(not used in the end)\n",
    "    latent_space = l\n",
    "\n",
    "    # Create decoding layers using functional API. \n",
    "    l2 = Dense(128, activation='relu')(l)\n",
    "    l2 = Dense(256, activation='relu')(l2)\n",
    "    l2 = Dense(output_dims, activation='relu')(l2)\n",
    "\n",
    "    # Store reference to final output.\n",
    "    out_layer = l2\n",
    "\n",
    "    auto_model = Model(input=inputLayer, output=out_layer)  # Create model.\n",
    "    \n",
    "    # Set SGD learning rate and compile model with MSE as loss function.\n",
    "    sgd = SGD(lr=0.1)\n",
    "    auto_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "    # Train the model for the specified number of epochs, using the specified validation fraction.\n",
    "    autoencoder_train = auto_model.fit(x_train, x_train, shuffle=False, validation_split = val_fraction, epochs=epochs)\n",
    "    \n",
    "    # Once traning is done, plot model loss and validation loss\n",
    "    # This is a visual guide for how well the model is learning (is it overfitting, underfitting?)\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model  # Return the model, now trained.\n",
    "\n",
    "# An encoder which learns to turn fragment spectra into CDK fingerprints for the same molecule.\n",
    "# Structure: (Input)1000-256-128-16-128-256-307(Output)\n",
    "def fingerprint_autoencoder(x_train_spectra, x_train_fingerprints, epochs=100, lr=0.5):\n",
    "    # Create input based on the provided x_train data structure.\n",
    "    input_layer = Input(shape=(x_train_spectra.shape[1],))\n",
    "    # Since output is not the same as input, we obtain its shape separately.\n",
    "    output_dims = x_train_fingerprints.shape[1]\n",
    "    \n",
    "    # Create the encoding layers using functional API.\n",
    "    l = input_layer\n",
    "    l = Dense(256, activation='relu')(l)\n",
    "    l = Dense(128, activation='relu')(l)\n",
    "    l = Dense(16, activation='relu')(l)\n",
    "    \n",
    "    # Create reference to latent space\n",
    "    latent_space = l\n",
    "    \n",
    "    # Create decoding layers using functional API.\n",
    "    l2 = Dense(128, activation='relu')(l)\n",
    "    # Linear activation function followed by sigmoid to get outputs between 0 and 1.\n",
    "    # This is done because the output fingerprint is a set of bits (0 or 1).\n",
    "    # Linear activation ensures that values can be negative (necessary for sigmoid to function)\n",
    "    l2 = Dense(256, activation='linear')(l2)\n",
    "    l2 = Dense(output_dims, activation='sigmoid')(l2)\n",
    "    \n",
    "     # Store reference to final output.\n",
    "    out_layer = l2\n",
    "    \n",
    "    auto_model = Model(input=input_layer, output=out_layer)\n",
    "    \n",
    "    # Set SGD learning rate provided as parameter and compile model with MSE as loss function.\n",
    "    sgd = SGD(lr=lr)\n",
    "    auto_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    # Train the model for the specified number of epochs, using the specified validation fraction.\n",
    "    autoencoder_train = auto_model.fit(x_train_spectra, x_train_fingerprints, shuffle=False, validation_split = val_fraction, epochs=epochs)\n",
    "    \n",
    "    # Once traning is done, plot model loss and validation loss\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model # Return the model, now trained.\n",
    "\n",
    "# A simplified spectrum-fingeprrint encoder.\n",
    "# Structure: (Input)1000-500-200-307(Output)\n",
    "def simplified_fingerprint_autoencoder(x_train_spectra, x_train_fingerprints, epochs=100, lr=0.5):\n",
    "    # Create input based on the provided x_train data structure.\n",
    "    input_layer = Input(shape=(x_train_spectra.shape[1],))\n",
    "    # Since output is not the same as input, we obtain its shape separately.\n",
    "    output_dims = x_train_fingerprints.shape[1]\n",
    "    \n",
    "    # Create the encoding layers using functional API.\n",
    "    l = input_layer\n",
    "    l = Dense(500, activation='relu')(l)\n",
    "    \n",
    "    # Linear activation ensures that values can be negative (necessary for sigmoid to function)\n",
    "    l = Dense(200, activation='linear')(l)\n",
    "    \n",
    "    # Save reference to latent space\n",
    "    latent_space = l\n",
    "    \n",
    "    # Sigmoid activation to get outputs between 0 and 1. This is done because the output fingerprint is a set of bits (0 or 1).\n",
    "    l2 = Dense(output_dims, activation='sigmoid')(l)\n",
    "    \n",
    "    #Reference for output layer\n",
    "    out_layer = l2\n",
    "\n",
    "    auto_model = Model(input=input_layer, output=out_layer)\n",
    "    \n",
    "    # Set SGD learning rate provided as parameter and compile model with MSE as loss function.\n",
    "    sgd = SGD(lr=lr)\n",
    "    auto_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    # Train the model for the specified number of epochs, using the specified validation fraction.\n",
    "    autoencoder_train = auto_model.fit(x_train_spectra, x_train_fingerprints, shuffle=False, validation_split = 0.1, epochs=epochs)\n",
    "    \n",
    "    # Loss Plots\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model # Return model, now trained\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creates and trains spectrum-fingerprint encoder which combines Dense layers with a single Conv1D layer using a Merge Layer\n",
    "# A hybrid neural network: requires both normal and convolutional spectra as inputs: convolutional has 1 more dimension\n",
    "# Allows for specifying training epochs and kernel size(filter width). Number of filters is kept constant\n",
    "def conv_autoencoder(x_train_conv, x_train_dense, x_train_fingerprints, epochs=100, kernel_size=20):\n",
    "    # Grab input and output shapes\n",
    "    conv_input = Input(shape=(x_train_conv.shape[1], 1))\n",
    "    dense_input = Input(shape=(x_train_dense.shape[1],))\n",
    "    output_dims = x_train_fingerprints.shape[1]\n",
    "    \n",
    "    # 1D convolution on spectra\n",
    "    c = conv_input\n",
    "    # Name layer in case weights need to be extracted.\n",
    "    c = Conv1D(32, kernel_size, activation='relu', padding='valid', name='conv_layer')(conv_input)\n",
    "    f = Flatten()(c) # Flatten for merging\n",
    "    \n",
    "    # Dense layer on spectra\n",
    "    d = dense_input\n",
    "    d = Dense(500, activation='relu')(d)\n",
    "    \n",
    "    # Merge convolution and dense layers\n",
    "    m = concatenate(([f, d]))\n",
    "    # Linear activation layer\n",
    "    m = Dense(200, activation='linear')(m)\n",
    "    \n",
    "    # Save reference to latent space\n",
    "    latent_space = m\n",
    "    # Final sigmoid layer to get fingerprint output.\n",
    "    l2 = Dense(output_dims, activation='sigmoid')(m)\n",
    "    \n",
    "    out_layer = l2\n",
    "    \n",
    "    auto_model = Model(inputs=[conv_input, dense_input], output=out_layer)\n",
    "    \n",
    "    # Set SGD learning rate, compile model with MSE as loss function\n",
    "    sgd = SGD(lr=0.5)\n",
    "    auto_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    # Train model using specified number of epochs and validationf fraction.\n",
    "    autoencoder_train = auto_model.fit([x_train_conv, x_train_dense], x_train_fingerprints, shuffle=False, validation_split = 0.1, epochs=epochs)\n",
    "    \n",
    "    # Plot loss\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model\n",
    "\n",
    "# Creates and trains a fingerprint-spectrum encoder which uses only a convolution layer\n",
    "# Note that the input must be ready for the Conv1D layer: needs an additional dimension added.\n",
    "# Allows for specifying training epochs and kernel size(filter width). Number of filters is kept constant\n",
    "def conv_only_autoencoder(x_train_conv, x_train_fingerprints, epochs=100, kernel_size=20):\n",
    "    # Grab input and output shapes\n",
    "    conv_input = Input(shape=(x_train_conv.shape[1], 1))\n",
    "    output_dims = x_train_fingerprints.shape[1]\n",
    "\n",
    "    # 1D convolution on spectra\n",
    "    c = conv_input\n",
    "    # Name layer in case we want to extract weights\n",
    "    c = Conv1D(32, kernel_size, activation='relu', padding='valid', name='conv_layer')(conv_input)\n",
    "    c = MaxPooling1D()(c)\n",
    "    c = Flatten()(c)\n",
    "\n",
    "    d = Dense(200, activation='linear')(c)\n",
    "\n",
    "    l2 = Dense(output_dims, activation='sigmoid')(d)\n",
    "\n",
    "    out_layer = l2\n",
    "\n",
    "    auto_model = Model(inputs=conv_input, output=out_layer)\n",
    "\n",
    "    #Set learning rate, compile model with mse\n",
    "    sgd = SGD(lr=0.5)\n",
    "    auto_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "    #Train for specified number of epochs\n",
    "    autoencoder_train = auto_model.fit(x_train_conv, x_train_fingerprints, shuffle=False,\n",
    "                                       validation_split=0.1, epochs=epochs)\n",
    "\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "\n",
    "    return auto_model\n",
    "\n",
    "# Creates and trains a hybrid model identical to conv_autoencoder but with an added MaxPooling1D layer.\n",
    "def conv_pool_autoencoder(x_train_conv, x_train_dense, x_train_fingerprints, epochs=100, kernel_size=20):\n",
    "    conv_input = Input(shape=(x_train_conv.shape[1], 1))\n",
    "    dense_input = Input(shape=(x_train_dense.shape[1],))\n",
    "    output_dims = x_train_fingerprints.shape[1]\n",
    "    \n",
    "    # 1D convolution on spectra\n",
    "    c = conv_input\n",
    "    print(kernel_size)\n",
    "    c = Conv1D(32, kernel_size, activation='relu', padding='valid', name='conv_layer')(conv_input)\n",
    "    c = MaxPooling1D()(c)\n",
    "    f = Flatten()(c) # Flatten for merging\n",
    "    \n",
    "    # Dense layer on spectra\n",
    "    d = dense_input\n",
    "    d = Dense(500, activation='relu')(d)\n",
    "    \n",
    "    # Merge convolution and dense layers\n",
    "    m = concatenate(([f, d]))\n",
    "    m = Dense(200, activation='linear')(m)\n",
    "    \n",
    "    latent_space = m\n",
    "    \n",
    "    l2 = Dense(output_dims, activation='sigmoid')(m)\n",
    "    \n",
    "    out_layer = l2\n",
    "    \n",
    "    auto_model = Model(inputs=[conv_input, dense_input], output=out_layer)\n",
    "    \n",
    "    # Set learning rate, compile.\n",
    "    sgd = SGD(lr=0.5)\n",
    "    auto_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    autoencoder_train = auto_model.fit([x_train_conv, x_train_dense], x_train_fingerprints, shuffle=False, validation_split = 0.1, epochs=epochs)\n",
    "    \n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input a trained neural network model and extracts its history variable.\n",
    "# It then uses it to graph the model's loss and validation loss over the training epochs\n",
    "# The epochs paramter is used for plotting the x axis.\n",
    "def plot_loss(fitted_model, epochs):\n",
    "    # Extract loss values for the training and validation sets.\n",
    "    loss = fitted_model.history['loss']\n",
    "    val_loss = fitted_model.history['val_loss']\n",
    "    # Create x axis variables.\n",
    "    epochs_label = epochs\n",
    "    epochs = range(epochs)\n",
    "\n",
    "    #Plot both losses.\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss,'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss for ' + str(epochs_label) + ' epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Takes a trained autoencoder model and input data. Uses the model to predit on the provided data\n",
    "# It then plots the input data vs. the model's prediction for a specified sample in the data.\n",
    "def plot_input_output(auto_model, data, sample=0):\n",
    "    # set font size\n",
    "    matplotlib.rcParams.update({'font.size': 14})\n",
    "    # Use model to make predictions for the provided input data.\n",
    "    decoded_data = auto_model.predict(data)\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, d in enumerate(data[sample]):\n",
    "        # For each datapoint in the specified sample, plot a vertical line equal to its intensity value.\n",
    "        if d>0.1:\n",
    "            ax.plot([i,i],[0, d], color='g')\n",
    "        # Do the same for the corresponding prediction but using negative values, to create a \"mirror\" plot\n",
    "        if decoded_data[sample][i]>0.1:\n",
    "            ax.plot([i,i],[0,-decoded_data[sample][i]], color='r')\n",
    "    # Plot invisible line on far end of spectrum so all 1000 mass bins are shown even if absent.\n",
    "    ax.plot([999,999], [0,0])\n",
    "    ax.set_xlabel(\"Mass Bin(Da)\")\n",
    "    ax.set_ylabel(\"Relative Abundance\")\n",
    "    plt.show()\n",
    "    \n",
    "# Takes actual and predicted fingerprint bit sets and plots a representative graph\n",
    "# Similar to the fragment spectrum plotting.\n",
    "def plot_fingerprint_output(actual, predicted, sample=0):\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, d in enumerate(actual[sample]):\n",
    "        # For each substructure in fingerprint, plot a vertical line if it is present.\n",
    "        ax.plot([i,i],[0,d], color='g')\n",
    "        # DO the same for prediction, using the predicted probability of the susbtructure's presence.\n",
    "        ax.plot([i,i],[0, -predicted[sample][i]], color = 'r')\n",
    "    # Plot invisible line on far end of fingerprint.\n",
    "    ax.plot([307,307], [0,0])\n",
    "    ax.set_xlabel(\"Substructure (Index in CDK)\")\n",
    "    ax.set_ylabel(\"Probability of Presence\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Takes a actual and predicted fingerprint values and computes the area under the Roc curve for each substructure.\n",
    "# For each subtructure, also calculates AUC when the actual values are scrambled.\n",
    "# Return two numpy arrays: one containing AUC metrics for all susbtructures, one containing each permutation's\n",
    "# AUC scores for each susbtructure.\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "def compute_auc(true, pred, permutations=500):\n",
    "    val_start_index = int(num_samples-(num_samples*val_fraction)-1) # Index where validation samples begin.\n",
    "    num_permutations = permutations  # Number of permutations to compute AUC scores for. \n",
    "    \n",
    "    # Create structured array to hold statistics for each fingerprint.\n",
    "    dtype = [('fp_id', int),('nonzeros', int), ('auc', float), ('auc_percent', float)]\n",
    "    mol_stats = np.zeros((320,), dtype=dtype)\n",
    "    \n",
    "    # Create array to hold permutation AUC scores for plotting.\n",
    "    perm_scores = np.zeros((320, num_permutations))\n",
    "    \n",
    "    for fp_id in range(true.shape[1]-1): # For every substructure      \n",
    "        nonzero_vals = np.count_nonzero(true[val_start_index:, fp_id]) # Count number of nonzero values\n",
    "        if nonzero_vals > 0 and nonzero_vals < true[val_start_index:, fp_id].size:  # If there are no 1s or no 0s, can't compute.\n",
    "            # Compute actual AUC score using only the validation fraction of the dataset.\n",
    "            fp_true = true[val_start_index:, fp_id]\n",
    "            fp_pred = pred[val_start_index:, fp_id]\n",
    "            score = metrics.roc_auc_score(fp_true, fp_pred)           \n",
    "\n",
    "            # Compute AUC scores for permutations and compare to actual.\n",
    "            counter = 0         \n",
    "            for i in range(num_permutations):\n",
    "                permutation = np.random.permutation(fp_true)\n",
    "                perm_score = metrics.roc_auc_score(permutation, fp_pred)\n",
    "                perm_scores[fp_id, i] = perm_score\n",
    "                # Count how many permutations have a higer AUC score than actual data.\n",
    "                if perm_score >= score:\n",
    "                    counter = counter + 1\n",
    "            # Calculate % of scrambled values with higher AUC score than actual AUC\n",
    "            percentage = (counter/num_permutations)*100\n",
    "        # Update structured array with data or non values if no AUC could be calculated.\n",
    "            mol_stats[fp_id] = fp_id, nonzero_vals, score, percentage\n",
    "        else:\n",
    "            mol_stats[fp_id] = (fp_id, nonzero_vals, 0, 100)\n",
    "        \n",
    "    # Permutations take a while, print statement to say when finished.\n",
    "    print(\"Done\")\n",
    "    return mol_stats, perm_scores\n",
    "\n",
    "\n",
    "# Takes a set of AUC scores and permutation AUC scores (normally output by compute_auc above) an uses them\n",
    "# to draw boxplots for specified susbtructures. Actual AUC is plotted as a coloured dot.\n",
    "plt.rcParams['figure.dpi'] = default_dpi*2.2\n",
    "def boxplots(real_stats, perm_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(fingerprint_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "\n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures in colour.\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Takes a set of AUC scores and permutation AUC scores and uses them to draw boxplots for specified substructures\n",
    "# Actual AUC is plotted as a coloured dot. A separate set of AUC scores\n",
    "# computed for prediction from a different model is also plotted for comparison\n",
    "def tandem_boxplots(real_stats, perm_stats, exp_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(fingerprint_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "  \n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures\n",
    "    plt.scatter(exp_stats[index]['auc'], range(1, len(index)+1), color = 'r') # Scatter plot AUC scores to be compared to.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Given the AUC statistics derived from two separate models, it comapres the two models' performance\n",
    "# Creates a bar chart comparing substructures above an AUC threshold and draws boxplots for each model's best and worst\n",
    "# performing substructures.\n",
    "# Usually compares an experimental model's AUC to a baseline (e.g. the basic fingerprint encoder)\n",
    "def evaluate(base_stats, base_perm_scores, exp_stats, exp_perm_scores):\n",
    "    # Sort molecules in ascending order of baseline AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((base_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((base_stats['auc']) < 0.5)\n",
    "    ordered_base = np.sort(base_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_base[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_base[-5:])\n",
    "    \n",
    "    # Plot number of substructures with AUC scores above 0.7 and above 0.5 for both data sets\n",
    "    base_above_07 = len(np.where((base_stats['auc'] >= 0.7))[0])\n",
    "    exp_above_07 = len(np.where((exp_stats['auc'] >= 0.7))[0])\n",
    "    base_above_05 = len(np.where((base_stats['auc'] >= 0.5))[0])\n",
    "    exp_above_05 = len(np.where((exp_stats['auc'] >= 0.5))[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(2)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.5\n",
    "    ax.bar(index, (base_above_05, base_above_07), bar_width, alpha=opacity, color='b', label='Baseline')\n",
    "    ax.bar(index+bar_width, (exp_above_05, exp_above_07), bar_width, alpha=opacity, color='r', label='Experiment')\n",
    "    \n",
    "    ax.set_xlabel('AUC Threshold')\n",
    "    ax.set_ylabel('Number of Substructures')\n",
    "    ax.set_title('AUC Score Comparison')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(('Above 0.5', 'Above 0.7'))\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplots of sample substructures for both data sets\n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, base_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "     # Sort molecules in ascending order of experimental AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((exp_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((exp_stats['auc']) < 0.5)\n",
    "    ordered_exp = np.sort(exp_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_exp[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_exp[-5:])\n",
    "    \n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, exp_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "\n",
    "# Given a matrix of layer weights, plots them in a Hinton diagram: each weight is a box\n",
    "# Box size is indicates absolute value, box colour indicates sign (white for positive, black for negative)\n",
    "# Adapted from matplotlib documentation.\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    # Find maximum weight in matrix.\n",
    "    if not max_weight:\n",
    "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    # Plot weights as black or white boxes.\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w) / max_weight)\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are quick ways to train multiple times using different training-validation splits. Used when we want means, error bars and statistical tests for model comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the basic fingerprint encoder using 10 different validation-training splits.\n",
    "# Computes AUC scores for each split and stores them as a separate file.\n",
    "# Takes path to store files in as a parameter, as well as the name of the test.\n",
    "def train_diff_splits(path, name, splits=10):\n",
    "    # Extract permuted indices for dataset.\n",
    "    index_path = \"E:\\\\Development Project\\\\Data\\\\Validation Spit Permutations.txt\"\n",
    "    permuted_indices = np.loadtxt(index_path, dtype=int, delimiter=',')\n",
    "    epochs = 100\n",
    "    path = path + name + \" \"\n",
    "    #List to store AUC scores if we want to use them right away instead of loading from files.\n",
    "    experiment_stats = []\n",
    "    for i in range(splits):\n",
    "        # Create filepath for this training session.\n",
    "        curr_path = path + str(i) + \".txt\"\n",
    "        # Use permuted indices to create permuted array of input data.\n",
    "        x_train_dense = spectra[permuted_indices[:, i]]\n",
    "        x_train_dense = np.log(x_train_dense+1)\n",
    "        x_train_fingerprints = fingerprints[permuted_indices[:, i]]\n",
    "        # Train a basic model.\n",
    "        enc_basic = fingerprint_autoencoder(x_train_dense, x_train_fingerprints, epochs=100)\n",
    "        # Use trained model to compute AUC scores for substructures and save them to disc.\n",
    "        actual = x_train_fingerprints\n",
    "        predicted = enc_basic.predict(x_train_dense)\n",
    "        base_stats, base_perm_scores = compute_auc(actual, predicted)\n",
    "        \n",
    "        np.savetxt(curr_path, base_stats, fmt=['%d', '%d', '%f', '%f'])\n",
    "        experiment_stats.append(base_stats)\n",
    "    return experiment_stats\n",
    "\n",
    "# Trains the convolution-only fingerprint encoder using 10 different validation-training splits.\n",
    "# Computes AUC scores for each split and stores them as a separate file.\n",
    "# Takes path to store files in as a parameter, as well as the name of the test.\n",
    "def train_diff_splits_conv(path, name, kernel_size=20, splits=10):\n",
    "     # Extract permuted indices for dataset.\n",
    "    index_path = \"E:\\\\Development Project\\\\Data\\\\Validation Spit Permutations.txt\"\n",
    "    permuted_indices = np.loadtxt(index_path, dtype=int, delimiter=',')\n",
    "    epochs = 75\n",
    "    path = path + name + \" \"\n",
    "    #List to store AUC scores if we want to use them right away instead of loading from files.\n",
    "    experiment_stats = []\n",
    "    for i in range(splits):\n",
    "        # Use permuted indices to create permuted array of input data.\n",
    "        x_train_dense = spectra[permuted_indices[:, i]]\n",
    "        x_train_dense = np.log(x_train_dense+1)\n",
    "        \n",
    "        # Add dimension for Conv1D layer\n",
    "        x_train_conv = np.expand_dims(x_train_dense, axis=2)\n",
    "        x_train_fingerprints = fingerprints[permuted_indices[:, i]]\n",
    "        \n",
    "        # Train convolutiona model.\n",
    "        enc_conv_only = conv_only_autoencoder(x_train_conv, x_train_fingerprints, kernel_size=kernel_size, epochs=epochs)\n",
    "        \n",
    "        # Use trained model to compute AUC scores for substructures and save them to disc.\n",
    "        actual = x_train_fingerprints\n",
    "        predicted = enc_conv_only.predict(x_train_conv)\n",
    "        exp_stats, exp_perm_scores = compute_auc(actual, predicted)\n",
    "        \n",
    "        # Create filepath for this training session.\n",
    "        curr_path = path + str(i) + \".txt\"\n",
    "        np.savetxt(curr_path, exp_stats, fmt=['%d', '%d', '%f', '%f'])\n",
    "        experiment_stats.append(exp_stats)\n",
    "    return experiment_stats\n",
    "\n",
    "# Trains the hybrid fingerprint encoder using 10 different validation-training splits.\n",
    "# Computes AUC scores for each split and stores them as a separate file.\n",
    "def train_diff_splits_hybrid(path, name, kernel_size=20, splits=10):\n",
    "    # Extract permuted indices for dataset.\n",
    "    index_path = \"E:\\\\Development Project\\\\Data\\\\Validation Spit Permutations.txt\"\n",
    "    permuted_indices = np.loadtxt(index_path, dtype=int, delimiter=',')\n",
    "    epochs = 100\n",
    "    path = path + name + \" \"\n",
    "    #List to store AUC scores if we want to use them right away instead of loading from files.\n",
    "    experiment_stats = []\n",
    "    for i in range(splits):\n",
    "        # Use permuted indices to create permuted array of input data.\n",
    "        x_train_dense = spectra[permuted_indices[:, i]]\n",
    "        x_train_dense = np.log(x_train_dense+1)\n",
    "        \n",
    "        # Add dimension for Conv1D layer\n",
    "        x_train_conv = np.expand_dims(x_train_dense, axis=2)\n",
    "        x_train_fingerprints = fingerprints[permuted_indices[:, i]]\n",
    "        \n",
    "        # Train hybrid model\n",
    "        enc_hybrid = conv_autoencoder(x_train_conv, x_train_dense, x_train_fingerprints, kernel_size=kernel_size, epochs=epochs)\n",
    "        \n",
    "        # Use trained model to compute AUC scores for substructures and save them to disc.\n",
    "        actual = x_train_fingerprints\n",
    "        predicted = enc_hybrid.predict([x_train_conv, x_train_dense])\n",
    "        exp_stats, exp_perm_scores = compute_auc(actual, predicted)\n",
    "        \n",
    "        # Create filepath for this training session.\n",
    "        curr_path = path + str(i) + \".txt\"\n",
    "        np.savetxt(curr_path, exp_stats, fmt=['%d', '%d', '%f', '%f'])\n",
    "        experiment_stats.append(exp_stats)\n",
    "    return experiment_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the basic set up for running any of the methods. It loads the fragment spectra and fingerprints for all molecules, as well spectr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CCMSLIB00000001548', 'CCMSLIB00000001549', 'CCMSLIB00000001550',\n",
      "       'CCMSLIB00000001555', 'CCMSLIB00000001563', 'CCMSLIB00000001565',\n",
      "       'CCMSLIB00000001566', 'CCMSLIB00000001568', 'CCMSLIB00000001569',\n",
      "       'CCMSLIB00000001570',\n",
      "       ...\n",
      "       'CCMSLIB00000579916', 'CCMSLIB00000579917', 'CCMSLIB00000579918',\n",
      "       'CCMSLIB00000579919', 'CCMSLIB00000579920', 'CCMSLIB00000579921',\n",
      "       'CCMSLIB00000579922', 'CCMSLIB00000579923', 'CCMSLIB00000579924',\n",
      "       'CCMSLIB00000579925'],\n",
      "      dtype='object', length=5770)\n",
      "Index(['CCMSLIB00000001548', 'CCMSLIB00000001549', 'CCMSLIB00000001550',\n",
      "       'CCMSLIB00000001555', 'CCMSLIB00000001563', 'CCMSLIB00000001565',\n",
      "       'CCMSLIB00000001566', 'CCMSLIB00000001568', 'CCMSLIB00000001569',\n",
      "       'CCMSLIB00000001570',\n",
      "       ...\n",
      "       'CCMSLIB00000579916', 'CCMSLIB00000579917', 'CCMSLIB00000579918',\n",
      "       'CCMSLIB00000579919', 'CCMSLIB00000579920', 'CCMSLIB00000579921',\n",
      "       'CCMSLIB00000579922', 'CCMSLIB00000579923', 'CCMSLIB00000579924',\n",
      "       'CCMSLIB00000579925'],\n",
      "      dtype='object', length=5770)\n",
      "(96222, 3)\n",
      "<U25\n",
      "Index(['CCMSLIB00000001548', 'CCMSLIB00000001549', 'CCMSLIB00000001550',\n",
      "       'CCMSLIB00000001555', 'CCMSLIB00000001563', 'CCMSLIB00000001565',\n",
      "       'CCMSLIB00000001566', 'CCMSLIB00000001568', 'CCMSLIB00000001569',\n",
      "       'CCMSLIB00000001570',\n",
      "       ...\n",
      "       'CCMSLIB00000579916', 'CCMSLIB00000579917', 'CCMSLIB00000579918',\n",
      "       'CCMSLIB00000579919', 'CCMSLIB00000579920', 'CCMSLIB00000579921',\n",
      "       'CCMSLIB00000579922', 'CCMSLIB00000579923', 'CCMSLIB00000579924',\n",
      "       'CCMSLIB00000579925'],\n",
      "      dtype='object', length=5770)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319]\n",
      "                    0    1    2    3    4    5    6    7    8    9   ...   \\\n",
      "CCMSLIB00000001548    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001549    1    1    0    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001550    0    0    0    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001555    1    1    1    0    0    0    0    1    0    0 ...    \n",
      "CCMSLIB00000001563    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001565    1    1    1    0    1    1    0    0    0    0 ...    \n",
      "CCMSLIB00000001566    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001568    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001569    1    1    1    0    1    1    0    0    0    0 ...    \n",
      "CCMSLIB00000001570    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001572    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001574    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001576    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001581    1    1    1    1    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001590    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001598    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001600    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001601    1    1    1    1    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001602    1    1    1    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001603    1    1    1    1    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001604    1    1    1    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001606    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001607    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001608    1    1    1    1    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001609    1    1    0    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001615    1    1    1    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001616    1    1    1    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001617    1    1    1    0    1    0    0    1    0    0 ...    \n",
      "CCMSLIB00000001621    0    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000001622    0    1    0    0    0    0    0    0    0    0 ...    \n",
      "...                 ...  ...  ...  ...  ...  ...  ...  ...  ...  ... ...    \n",
      "CCMSLIB00000579896    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579897    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579898    1    0    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579899    1    1    1    1    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579900    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579901    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579902    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579903    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579904    1    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579905    1    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579906    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579907    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579908    0    0    0    0    0    0    0    0    1    0 ...    \n",
      "CCMSLIB00000579909    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579910    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579911    1    0    0    0    0    0    0    0    1    0 ...    \n",
      "CCMSLIB00000579912    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579913    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579914    1    1    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579915    1    0    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579916    1    0    0    1    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579917    0    0    0    0    0    0    0    0    1    0 ...    \n",
      "CCMSLIB00000579918    1    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579919    1    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579920    0    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579921    1    1    1    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579922    0    0    0    0    1    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579923    1    0    0    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579924    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "CCMSLIB00000579925    1    1    1    0    0    0    0    0    0    0 ...    \n",
      "\n",
      "                    310  311  312  313  314  315  316  317  318  319  \n",
      "CCMSLIB00000001548    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001549    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001550    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001555    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001563    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001565    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001566    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001568    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001569    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001570    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001572    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001574    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001576    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001581    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001590    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001598    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001600    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001601    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001602    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001603    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001604    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001606    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001607    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001608    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001609    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001615    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001616    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001617    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001621    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000001622    0    0    0    0    0    0    0    0    0    0  \n",
      "...                 ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "CCMSLIB00000579896    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579897    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579898    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579899    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579900    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579901    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579902    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579903    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579904    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579905    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579906    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579907    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579908    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579909    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579910    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579911    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579912    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579913    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579914    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579915    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579916    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579917    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579918    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579919    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579920    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579921    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579922    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579923    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579924    0    0    0    0    0    0    0    0    0    0  \n",
      "CCMSLIB00000579925    0    0    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[5770 rows x 320 columns]\n"
     ]
    }
   ],
   "source": [
    "spectra = load_master_file(path=datapath)\n",
    "fingerprints = load_fingerprints_master()\n",
    "fingerprint_names = load_fingerprint_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to run a fingerprint encoder\n",
    "epochs = 100\n",
    "\n",
    "x_train_spectra = np.log(spectra+1) \n",
    "x_train_fingerprints = fingerprints\n",
    "enc = basic_autoencoder(x_train_spectra, x_train_fingerprints, epochs=epochs)\n",
    "\n",
    "actual = x_train_fingerprints\n",
    "predicted = enc.predict(x_train_spectra)\n",
    "baseline_stats, baseline_perm_scores = compute_auc(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to run a hybrid encoder\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "x_train_dense = spectra\n",
    "x_train_dense = np.log(x_train_dense+1)\n",
    "x_train_fingerprints = fingerprints\n",
    "\n",
    "# Add dimension for Conv1D layer\n",
    "x_train_conv = np.expand_dims(x_train_dense, axis=2)\n",
    "\n",
    "x_train_fingerprints = fingerprints\n",
    "\n",
    "enc_hybrid = conv_autoencoder(x_train_conv, x_train_dense, x_train_fingerprints)\n",
    "\n",
    "actual = x_train_fingerprints\n",
    "predicted = enc_hybrid.predict([x_train_conv, x_train_dense]) # Note how a list of inputs is used, one dense, one convolutional\n",
    "exp_stats, exp_perm_scores = compute_auc(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic comparisons of the two trained models\n",
    "evaluate(baseline_stats, baseline_perm_scores, exp_stats, exp_perm_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
